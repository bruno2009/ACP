---
title: "<center><div class='mytitle'>Principal Component Analysis</div></center>"
output:
  html_document:
      highlight: arrow
      css: style.css
      toc: FALSE
      #math_method: 
        #engine: webtex
        #url: https://latex.codecogs.com/svg.image?
      citation_package: biblatex
      #includes: 
         #in_header: Capa.html
         #after_body: Footer.html
header-includes:
  - \usepackage{amsmath}
bibliography: yourBibFile.bib
link-citations: true
biblio-style: authoryear
---



```{r setup, include=FALSE, echo=FALSE}

knitr::opts_chunk$set(tidy.opts=list( width.cutoff = 30 ), tidy=T)

```

<section class="section" >
  <div class="content">

This topic will discuss the mathematics of  Eigenvectors and Eigenvalues, Spectral Decomposition Theorem, and Principal Component Analysis. Moreover, I will show an application of this technique. 

# Eigenvectors and Eigenvalues

<div class="Definition">
<p><strong>Definition 1</strong>.
Let a non-null vector $v$ (i.e. $\vec{v} \neq 0$), $v \in V$ (vector space) is said a eigenvectors of $T$ if there exist a real number $\lambda$ which $T(v) = \lambda v$. The $\lambda$ scalar is denominated by an eigenvalue of $T$ associated with $v$. It can be concluded that $v$ and $T$ have the same support line (i.e. the linear transformation $T$ maps vector $v$ into the same direction):
</p>
</div>


<div class="equation">
  $$\begin{aligned}
  T: v \rightarrow v \\[5px]
  \Rightarrow
  T(\vec{v}) = \lambda \vec{v} \\[5px]
  \Rightarrow A\vec{v} = \lambda\vec{v}
  \end{aligned}$$
</div>

<div class="Preposition">
<p><strong>Preposition 1</strong>.
The eigenvectors of $A$ are precisely the roots of the characteristic polynomial of $A$. 
</p>
</div>

$A \in \mathcal{M}_{p}(\mathbb{R})$ or $A \in \mathbb{R}^{p \times p}$ of transformation $T: \mathbb{R}^{p} \rightarrow \mathbb{R}^{p}$. Therefore 

<div style="text-align: center;">
  $\lambda \in \mathbb{R}$ is eigenvalues of $A$. 
  
  $\Leftrightarrow \exists$ $\vec{v} \neq 0$ such that $A\vec{v} = \lambda\vec{v}$
  
  $\Leftrightarrow A\vec{v} - \lambda\vec{v} = 0$ where $AI = A$
  
  $\Leftrightarrow (A - \lambda I)\vec{v} = 0$
</div>

in which $I$ is identity matrix. So this system has a non-trivial solution if, and only if, the determinant of the matrix $(A - \lambda I)$ is zero (a consequence of Cramer's Rule),

<div class="equation">
  $$\begin{cases}
      \vec{v} \neq 0\\
      \det(A - \lambda I) = 0
    \end{cases}$$
</div>

In this manner, $(A - \lambda I)$ is non-invertible and singular. If $\vec{v} \neq 0(A - \lambda I)^{-1} = 0$, so $(A - \lambda I)$ is invertible it has a trivial solution and is not what we want!

Example: Let A is a matrix, and now we will find its eigenvectors and eigenvalues.

<div class="equation">
$$\begin{bmatrix}
 2 & -1 \\
-1 &  2 
\end{bmatrix}$$
</div>

and knowing that $det(A - \lambda I) =0$. Then,

<div class="equation">
$$ \begin{bmatrix}
 2 - \lambda & -1 \\
-1 &  2 - \lambda 
\end{bmatrix} = (2 - \lambda)(2 - \lambda) - 1 = 0$$
$$\lambda^2 - 4 \lambda + 3 = 0$$
$$ (\lambda - 3)(\lambda - 1) = 0$$
</div>

The solutions for the eigenvalues are $\lambda = (3, 1)^{\top}$. For the eigenvector equal to $\lambda_{1} = 3$, we have

<div class="equation">
$$ \begin{bmatrix}
 2 - 3 & -1 \\
-1 &  2 - 3 
\end{bmatrix} = \begin{bmatrix}
 v_{1}  \\
 v_{2}
\end{bmatrix} = \vec{0}$$
</div>
<div class="equation">
$$ \begin{bmatrix}
 -1 & -1 \\
-1 &  -1 
\end{bmatrix} = \begin{bmatrix}
 v_{1}  \\
 v_{2}
\end{bmatrix} = \vec{0}$$
</div>
<div class="equation">
$$\begin{cases}
      - v_{1} - v_{2} = 0\\
      - v_{1} - v_{2} = 0
    \end{cases} \Rightarrow  v_{1} = - v_{2}$$
$$ (- v_{2}, v_{2})^{\top} = v_{2}(- 1, 1)^{\top}$$
$$ \lambda = 3 \sim \text{span}\bigg\{\begin{bmatrix}
 -1  \\
 1
\end{bmatrix}\bigg\} \Rightarrow \vec{v} = (-1, 1)^{\top}$$

</div>

For the eigenvector equal to $\lambda_{1} = 1$, we have

<div class="equation">
$$ \begin{bmatrix}
 2 - 1 & -1 \\
-1 &  2 - 1 
\end{bmatrix} = \begin{bmatrix}
 v_{1}  \\
 v_{2}
\end{bmatrix} = \vec{0}$$
</div>
<div class="equation">
$$ \begin{bmatrix}
 1 & -1 \\
-1 &  1 
\end{bmatrix} = \begin{bmatrix}
 v_{1}  \\
 v_{2}
\end{bmatrix} = \vec{0}$$
</div>
<div class="equation">
$$\begin{cases}
        v_{1} - v_{2} = 0\\
      - v_{1} + v_{2} = 0
    \end{cases} \Rightarrow  v_{1} = v_{2}$$
$$ ( v_{2}, v_{2})^{\top} = v_{2}(1, 1)^{\top}$$
$$ \lambda = 1 \sim \text{span}\bigg\{\begin{bmatrix}
 1  \\
 1
\end{bmatrix}\bigg\} \Rightarrow \vec{v} = (1, 1)^{\top}$$

</div>

and normalizing the vectors to length 1, known as the unit vector; they are

<div class="equation">
$$ \frac{\vec{v}_{1}}{||\vec{v}_{1}||} = \begin{bmatrix}
 -1/\sqrt{2} \\
 1/\sqrt{2}
\end{bmatrix} $$
</div>
<div class="equation">
$$ \frac{\vec{v}_{2}}{||\vec{v}_{2}||} = \begin{bmatrix}
 1/\sqrt{2} \\
 1/\sqrt{2}
\end{bmatrix} $$
</div>

# Spectral Decomposition Theorem

<div class="Reminder">
<p><strong>Reminder:</strong>
I will discuss only the Spectral Decomposition Theorem, but other methods could be used as Singular value decomposition.
</p>
</div>

An asymmetric matrix is a matrix that is equal to its transpose. Thus, 

<div class="equation">
$$ A \in \mathcal{M}_{p}(\mathbb{R}) $$
$$ \Rightarrow A^{\top} \in \mathcal{M}_{p}(\mathbb{R}) $$
$$\Rightarrow A = A^{\top}$$
</div>


<div class="Teorema">
<p><strong>Spectral Theorem (Symmetric matrix):</strong>
Let $A \in \mathbb{R}^{p \times p}$, then
</p>
</div>

<div class="lista">
  <ol type="I">
  <li> The eigenvalues $\lambda_{1}, \dots, \lambda_{p}$ are real;</li>
  <li> The eigenvectors of A associated with different eigenvalues are orthogonal to each other. **Then:**

<div class="equation">
$$\begin{aligned}
<\vec{v}_{i}, \vec{v}_{j}> &= \vec{v}_{i}^{\top} \vec{v}_{j} \\[10pt]
 &=  \sum_{k=1}^{p} \vec{v}_{ki}\vec{v}_{kj}  \\[10pt]
 &= \vec{v}_{1i}\vec{v}_{1j} + \dots+ \vec{v}_{pi}\vec{v}_{pj} \\[10pt]
 &= 0
\end{aligned}$$
</div>
with $\vec{v}_{j} = (v_{1j}, \dots, v_{pj})^{\top} \in \mathbb{R}^{p \times 1}$, $i \neq j$ and $i, j, k = 1, \dots, p$; 

<div class="Preposition">
<p><strong>Preposition 2</strong>.
An orthogonal set of non-zero vectors is linearly independent.
</p>
</div>
<div class="Reminder">
<p><strong>Reminder:</strong>

Linearly independent $\nrightarrow$ Orthogonal

Linearly independent $\leftarrow$ Orthogonal
</p>
</div>

</li>
<li> It is always possible to obtain a basis $\{e_{1}, \dots, e_{p}\}$ of orthonormal eigenvectors. **Then:**

<div class="equation">
$$ \begin{matrix}
 \cfrac{\vec{v}_{1}}{||\vec{v}_{1}||} = (e_{11}, e_{21}, \dots, e_{p1})^{\top} = e_{1} \\
  \cfrac{\vec{v}_{2}}{||\vec{v}_{2}||} = (e_{12}, e_{22}, \dots, e_{p2})^{\top} = e_{1} \\
  \vdots\\
 \cfrac{\vec{v}_{p}}{||\vec{v}_{p}||} = (e_{1p}, e_{2p}, \dots, e_{pp})^{\top} = e_{p}
\end{matrix}$$
</div>

<div class="equation">
$$ e_{i}^{\top} e_{j} = \delta_{ij} =   \begin{cases}
      0 & \text{if } i \neq j\\
      1 & \text{if } i = j
    \end{cases}$$
</div>

and $i, j = 1, \dots, p$;

</li>
<li> The relation is valid $P^{-1}AP = D$, so $P$ is defined as follows 

<div class="equation">
$$ P_{p \times p} =\begin{bmatrix}
 e_{1} & \dots & e_{p} \\
\end{bmatrix} = 
\begin{bmatrix}
 e_{11} &  e_{12} & \dots & e_{1p} \\
 e_{21} &  e_{22} & \dots & e_{2p} \\
 \vdots &  \vdots & \ddots & \vdots \\
 e_{p1} &  e_{p2} & \dots & e_{pp} \\
\end{bmatrix}$$ 
</div>

where $P$ is a orthonormal eigenvectors matrix.
<div class="Reminder">
<p><strong>Reminder:</strong>
Orthonormal eigenvectors matrix has the following properties
$P^{\top}P = I = PP^{\top}$ and $P^{-1} = P^{\top}$.
</p>
</div>

and $D$ matrix is defined by 

<div class="equation">
$$ D_{p \times p} =
\begin{bmatrix}
\lambda_{1} &         &       & \\
        &  \lambda_{2} &       & \\
        &         &\ddots & \\
        &         &       & \lambda_{p}\\
\end{bmatrix}$$ 
</div>

where $D$ is a diagonal matrix in which eigenvalues are diagonal elements;

</li>
</ol> 
</div>

<div class="Definition">
<p><strong>Definition 2</strong>. A quadratic form in $\mathbb{R}^{n}$ is a function $Q: \mathbb{R}^{p} \rightarrow \mathbb{R}$ of form $Q(x) = x^{t}Ax$.
</p>
</div>

<div class="lista">
  <ol type="I">
    <li> $A$ is positive definite $\Leftrightarrow$ If $Q(x) > 0$, $\forall$ $x \in \mathbb{R}_{\small\setminus0}^{p}$.</li>
    <li> $A$ is positive semidefinite $\Leftrightarrow$ If $Q(x) \geq 0$, $\forall$ $x \in \mathbb{R}_{\small\setminus0}^{p}$.</li>
  </ol> 
</div>

Hence

<div class="lista">
  <ol type="I">
    <li> $A$ is positive definite, so $\lambda_{j} > 0$ for all $j = 1, \dots, p$.</li>
    <li> $A$ is positive semidefinite, so $\lambda_{j} \geq 0$ for all $j = 1, \dots, p$.</li>
  </ol> 
</div>


<div class="Important">
<p><strong>Important:</strong> If $A \in \mathbb{R}^{p \times p}$ and positive definite.
<div class="lista">
<ol type="I">
<li> All elements on the main diagonal are positive.</li>
<li> All main submatrix $A_{k}$ is positive definite for all $k = 1, \dots, p$.</li>
<li> A is non-singular $(\text{det}(A) \neq 0)$. **Exemplo:**
<div class="equation">
$$A = \begin{bmatrix}
            a & b \\
            b & c \\
          \end{bmatrix} \Rightarrow \text{det}(A) = ad - bb$$
</div>
<div class="equation">
$$\Rightarrow A^{-1} = \cfrac{1}{\text{det}(A)}\begin{bmatrix}
            d & - b \\
            - b & a \\
          \end{bmatrix}$$
</div>
</li>
<li> $A^{-1}$ is positive definite.</li>
</ol> 
</div>
</p>
</div>







 
# REFERENCES

<div id="refs">
---
nocite: '@*'
---
</div>

</div>
</section>






---
title: "<center><div class='mytitle'>Principal Component Analysis</div></center>"
output:
  html_document:
      highlight: arrow
      css: style.css
      toc: FALSE
      #math_method: 
        #engine: webtex
        #url: https://latex.codecogs.com/svg.image?
      citation_package: biblatex
      #includes: 
         #in_header: Capa.html
         #after_body: Footer.html
header-includes:
  - \usepackage{amsmath}
bibliography: yourBibFile.bib
link-citations: true
biblio-style: authoryear
---



```{r setup, include=FALSE, echo=FALSE}

knitr::opts_chunk$set(tidy.opts=list( width.cutoff = 30 ), tidy=T)

```

<section class="section" >
  <div class="content">

This topic will discuss the mathematics of  Eigenvectors and Eigenvalues, Spectral Decomposition Theorem, and Principal Component Analysis. Moreover finally, I will show an application of this technique. 

# Eigenvectors and Eigenvalues

<div class="Definition">
<p><strong>Definition 1</strong>.
Let a non-null vector $v$ (i.e. $\vec{v} \neq 0$), $v \in V$ (vector space) is said a eigenvectors of $T$ if there exist a real number $\lambda$ which $T(v) = \lambda v$. The $\lambda$ scalar is denominated by an eigenvalue of $T$ associated with $v$. It can be concluded that $v$ and $T$ have the same support line (i.e. the linear transformation $T$ maps vector $v$ into the same direction):
</p>
</div>


<div class="equation">
  $$\begin{aligned}
  T: v \rightarrow v \\[5px]
  \Rightarrow
  T(\vec{v}) = \lambda \vec{v} \\[5px]
  \Rightarrow A\vec{v} = \lambda\vec{v}
  \end{aligned}$$
</div>

<div class="Preposition">
<p><strong>Preposition 1</strong>.
The eigenvectors of $A$ are precisely the roots of the characteristic polynomial of $A$. 
</p>
</div>

$A \in \mathcal{M}_{p}(\mathbb{R})$ or $A \in \mathbb{R}^{p \times p}$ of transformation $T: \mathbb{R}^{p} \rightarrow \mathbb{R}^{p}$. Therefore 

<div style="text-align: center;">
  $\lambda \in \mathbb{R}$ is eigenvalues of $A$. 
  
  $\Leftrightarrow \exists$ $\vec{v} \neq 0$ such that $A\vec{v} = \lambda\vec{v}$
  
  $\Leftrightarrow A\vec{v} - \lambda\vec{v} = 0$ where $AI = A$
  
  $\Leftrightarrow (A - \lambda I)\vec{v} = 0$
</div>

in which $I$ is identity matrix. So this system has a non-trivial solution if, and only if, the determinant of the matrix $(A - \lambda I)$ is zero (a consequence of Cramer's Rule),

<div class="equation">
  $$\begin{cases}
      \vec{v} \neq 0\\
      \det(A - \lambda I) = 0
    \end{cases}$$
</div>

In this manner, $(A - \lambda I)$ is non-invertible and singular $(\vec{v} \neq 0(A - \lambda I)^{-1} = 0)$.

Example: Let A is a matrix, and now we will find its eigenvectors and eigenvalues.

<div class="equation">
$$\begin{bmatrix}
 2 & -1 \\
-1 &  2 
\end{bmatrix}$$
</div>

and knowing that $det(A - \lambda I) =0$. Then,

<div class="equation">
$$ \begin{bmatrix}
 2 - \lambda & -1 \\
-1 &  2 - \lambda 
\end{bmatrix} = (2 - \lambda)(2 - \lambda) - 1 = 0$$
$$\lambda^2 - 4 \lambda + 3 = 0$$
$$ (\lambda - 3)(\lambda - 1) = 0$$
</div>

The solutions for the eigenvalues are $\lambda = (3, 1)^{\top}$. For the eigenvector equal to $\lambda_{1} = 3$, we have

<div class="equation">
$$ \begin{bmatrix}
 2 - 3 & -1 \\
-1 &  2 - 3 
\end{bmatrix} = \begin{bmatrix}
 v_{1}  \\
 v_{2}
\end{bmatrix} = \vec{0}$$
</div>
<div class="equation">
$$ \begin{bmatrix}
 -1 & -1 \\
-1 &  -1 
\end{bmatrix} = \begin{bmatrix}
 v_{1}  \\
 v_{2}
\end{bmatrix} = \vec{0}$$
</div>
<div class="equation">
$$\begin{cases}
      - v_{1} - v_{2} = 0\\
      - v_{1} - v_{2} = 0
    \end{cases} \Rightarrow  v_{1} = - v_{2}$$
$$ (- v_{2}, v_{2})^{\top} = v_{2}(- 1, 1)^{\top}$$
$$ \lambda = 3 \sim \text{span}\bigg\{\begin{bmatrix}
 -1  \\
 1
\end{bmatrix}\bigg\} \Rightarrow \vec{v} = (-1, 1)^{\top}$$

</div>

For the eigenvector equal to $\lambda_{1} = 1$, we have

<div class="equation">
$$ \begin{bmatrix}
 2 - 1 & -1 \\
-1 &  2 - 1 
\end{bmatrix} = \begin{bmatrix}
 v_{1}  \\
 v_{2}
\end{bmatrix} = \vec{0}$$
</div>
<div class="equation">
$$ \begin{bmatrix}
 1 & -1 \\
-1 &  1 
\end{bmatrix} = \begin{bmatrix}
 v_{1}  \\
 v_{2}
\end{bmatrix} = \vec{0}$$
</div>
<div class="equation">
$$\begin{cases}
        v_{1} - v_{2} = 0\\
      - v_{1} + v_{2} = 0
    \end{cases} \Rightarrow  v_{1} = v_{2}$$
$$ ( v_{2}, v_{2})^{\top} = v_{2}(1, 1)^{\top}$$
$$ \lambda = 1 \sim \text{span}\bigg\{\begin{bmatrix}
 1  \\
 1
\end{bmatrix}\bigg\} \Rightarrow \vec{v} = (1, 1)^{\top}$$

</div>

and normalizing the vectors to length 1, known as the unit vector; they are

<div class="equation">
$$ \frac{\vec{v}_{1}}{||\vec{v}_{1}||} = \begin{bmatrix}
 -1/\sqrt{2} \\
 1/\sqrt{2}
\end{bmatrix} $$
</div>
<div class="equation">
$$ \frac{\vec{v}_{2}}{||\vec{v}_{2}||} = \begin{bmatrix}
 1/\sqrt{2} \\
 1/\sqrt{2}
\end{bmatrix} $$
</div>

# Spectral Decomposition Theorem

<div class="Reminder">
<p><strong>Reminder:</strong>
I will discuss only the Spectral Decomposition Theorem, but other methods could be used as Singular value decomposition.
</p>
</div>

An asymmetric matrix is a matrix that is equal to its transpose. Thus, 

<div class="equation">
$$ A \in \mathcal{M}_{p}(\mathbb{R}) $$
$$ \Rightarrow A^{\top} \in \mathcal{M}_{p}(\mathbb{R}) $$
$$\Rightarrow A = A^{\top}$$
</div>


<div class="Teorema">
<p><strong>Spectral Theorem (Symmetric matrix):</strong>
Let $A \in \mathbb{R}^{p \times p}$, so
</p>
</div>

<div class="lista">
  <ol type="a">
  <li> The eigenvalues $\lambda_{1}, \dots, \lambda_{p}$ are real;</li>
  <li> The eigenvectors of A associated with different eigenvalues are orthogonal to each other. **In other words:**

<div class="equation">
$$\begin{aligned}
<\vec{v}_{i}, \vec{v}_{j}> &= \vec{v}_{i}^{\top} \vec{v}_{j} \\[10pt]
 &=  \sum_{k=1}^{p} \vec{v}_{i_{k}}\vec{v}_{j_{k}}  \\[10pt]
 &= 0
\end{aligned}$$
</div>
with $\vec{v}_{j} = (v_{j_{1}}, \dots, v_{j_{p}})^{\top} \in \mathbb{R}^{p \times 1}$, $i \neq j$ and $i, j, k = 1, \dots, p$; 

</li>
<li> it is always possible to obtain a basis B from orthonormal eigenvectors;
</li>
  <li> $\mu - \sigma$ and $\mu + \sigma$ are inflection points of $f(x)$. </li>
  <li> The density of Normal Distribution is log-concave. </li>
  </ol> 
  </div>






<!--
The first one is the most famous distribution, is the Bernoulli distribution. Let $X$ a binary random variable with probability density function (PDF) $f_{x}$ . Then, $X \sim Ber(p)$ has PDF

<div class="equation">
  $$f(x) = \P(X = x) = p^{x} (1-p)^{1-x}$$
</div>
    
where the support is $X \in \{0, 1\}$ and parametric space is $p \in (0, 1)$. The expected value (mathematical expectation) is $\E(X) = p$ and variance is $\var(X) = p(1 - p)$.

I will not discuss moments in statistics here where the first moment is mathematical expectations and the second is related to variance. Nevertheless, [Wikipedia](https://en.wikipedia.org/wiki/Moment_(mathematics)) is a good site where you might start to study more about this topic. I love this concept because everything concerning statistical models is linked to a mean, mainly in generalized linear models ([MLG](https://en.wikipedia.org/wiki/Generalized_linear_model)). But it is a topic to see forward. 


<span style="width:40px;display:inline-block"></span>

```{r warning=FALSE, message=FALSE, fig.align="center", fig.height=4, class.source="watch-out"}
set.seed(123)

```
-->


 
# REFERENCES

<div id="refs">
---
nocite: '@*'
---
</div>

</div>
</section>






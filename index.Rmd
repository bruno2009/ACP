---
title: "<center><div class='mytitle'>Principal Component Analysis</div></center>"
output:
  html_document:
      highlight: arrow
      css: style.css
      toc: FALSE
      math_method: 
        engine: webtex
        url: https://latex.codecogs.com/svg.image?
      citation_package: biblatex
      includes: 
         in_header: Capa.html
         after_body: Footer.html
header-includes:
  - \usepackage{amsmath}
bibliography: yourBibFile.bib
link-citations: true
biblio-style: authoryear
---

\newcommand{\E}{\mathrm{E}}
\newcommand{\var}{\mathrm{Var}}
\newcommand{\cov}{\mathrm{Cov}}
\newcommand{\P}{\mathrm{P}}

```{r setup, include=FALSE, echo=FALSE}

knitr::opts_chunk$set(tidy.opts=list( width.cutoff = 30 ), tidy=T)

```

<section class="section" >
  <div class="content">

This topic will discuss the mathematics of  Eigenvectors and Eigenvalues, Spectral Decomposition Theorem, and Principal Component Analysis. Moreover, I will show an application of this technique. 

# EIGENVECTORS AND EIGENVALUES

<div class="Definition">
<p><strong>Definition 1</strong>.
Let a non-null vector $v$ (i.e. $\vec{v} \neq 0$), $v \in V$ (vector space) is said a eigenvectors of $T$ if there exist a real number $\lambda$ which $T(v) = \lambda v$. The $\lambda$ scalar is denominated by an eigenvalue of $T$ associated with $v$. It can be concluded that $v$ and $T$ have the same support line (i.e. the linear transformation $T$ maps vector $v$ into the same direction):
</p>
</div>


<div class="equation">
  $$\begin{aligned}
  T: v \rightarrow v \\[5px]
  \Rightarrow
  T(\vec{v}) = \lambda \vec{v} \\[5px]
  \Rightarrow A\vec{v} = \lambda\vec{v}
  \end{aligned}$$
</div>

<div class="Preposition">
<p><strong>Preposition 1</strong>.
The eigenvectors of $A$ are precisely the roots of the characteristic polynomial of $A$. 
</p>
</div>

$A \in \mathcal{M}_{p}(\mathbb{R})$ or $A \in \mathbb{R}^{p \times p}$ of transformation $T: \mathbb{R}^{p} \rightarrow \mathbb{R}^{p}$. Therefore 

<div style="text-align: center;">
  $\lambda \in \mathbb{R}$ is eigenvalues of $A$. 
  
  $\Leftrightarrow \exists$ $\vec{v} \neq 0$ such that $A\vec{v} = \lambda\vec{v}$
  
  $\Leftrightarrow A\vec{v} - \lambda\vec{v} = 0$ where $AI = A$
  
  $\Leftrightarrow (A - \lambda I)\vec{v} = 0$
</div>

in which $I$ is identity matrix. So this system has a non-trivial solution if, and only if, the determinant of the matrix $(A - \lambda I)$ is zero (a consequence of Cramer's Rule),

<div class="equation">
  $$\begin{cases}
      \vec{v} \neq 0\\
      \det(A - \lambda I) = 0
    \end{cases}$$
</div>

In this manner, $(A - \lambda I)$ is non-invertible and singular. If $\vec{v} \neq 0(A - \lambda I)^{-1} = 0$, so $(A - \lambda I)$ is invertible it has a trivial solution and is not what we want!

Example: Let A is a matrix, and now we will find its eigenvectors and eigenvalues.

<div class="equation">
$$\begin{bmatrix}
 2 & -1 \\
-1 &  2 
\end{bmatrix}$$
</div>

and knowing that $det(A - \lambda I) =0$. Then,

<div class="equation">
$$ \begin{bmatrix}
 2 - \lambda & -1 \\
-1 &  2 - \lambda 
\end{bmatrix} = (2 - \lambda)(2 - \lambda) - 1 = 0$$
$$\lambda^2 - 4 \lambda + 3 = 0$$
$$ (\lambda - 3)(\lambda - 1) = 0$$
</div>

The solutions for the eigenvalues are $\lambda = (3, 1)^{\top}$. For the eigenvector equal to $\lambda_{1} = 3$, we have

<div class="equation">
$$ \begin{bmatrix}
 2 - 3 & -1 \\
-1 &  2 - 3 
\end{bmatrix} = \begin{bmatrix}
 v_{1}  \\
 v_{2}
\end{bmatrix} = \vec{0}$$
</div>
<div class="equation">
$$ \begin{bmatrix}
 -1 & -1 \\
-1 &  -1 
\end{bmatrix} = \begin{bmatrix}
 v_{1}  \\
 v_{2}
\end{bmatrix} = \vec{0}$$
</div>
<div class="equation">
$$\begin{cases}
      - v_{1} - v_{2} = 0\\
      - v_{1} - v_{2} = 0
    \end{cases} \Rightarrow  v_{1} = - v_{2}$$
$$ (- v_{2}, v_{2})^{\top} = v_{2}(- 1, 1)^{\top}$$
$$ \lambda = 3 \sim \text{span}\bigg\{\begin{bmatrix}
 -1  \\
 1
\end{bmatrix}\bigg\} \Rightarrow \vec{v} = (-1, 1)^{\top}$$

</div>

For the eigenvector equal to $\lambda_{1} = 1$, we have

<div class="equation">
$$ \begin{bmatrix}
 2 - 1 & -1 \\
-1 &  2 - 1 
\end{bmatrix} = \begin{bmatrix}
 v_{1}  \\
 v_{2}
\end{bmatrix} = \vec{0}$$
</div>
<div class="equation">
$$ \begin{bmatrix}
 1 & -1 \\
-1 &  1 
\end{bmatrix} = \begin{bmatrix}
 v_{1}  \\
 v_{2}
\end{bmatrix} = \vec{0}$$
</div>
<div class="equation">
$$\begin{cases}
        v_{1} - v_{2} = 0\\
      - v_{1} + v_{2} = 0
    \end{cases} \Rightarrow  v_{1} = v_{2}$$
$$ ( v_{2}, v_{2})^{\top} = v_{2}(1, 1)^{\top}$$
$$ \lambda = 1 \sim \text{span}\bigg\{\begin{bmatrix}
 1  \\
 1
\end{bmatrix}\bigg\} \Rightarrow \vec{v} = (1, 1)^{\top}$$

</div>

and normalizing the vectors to length 1, known as the unit vector; they are

<div class="equation">
$$ \frac{\vec{v}_{1}}{||\vec{v}_{1}||} = \begin{bmatrix}
 -1/\sqrt{2} \\
 1/\sqrt{2}
\end{bmatrix} $$
</div>
<div class="equation">
$$ \frac{\vec{v}_{2}}{||\vec{v}_{2}||} = \begin{bmatrix}
 1/\sqrt{2} \\
 1/\sqrt{2}
\end{bmatrix} $$
</div>

# SPECTRAL DECOMPOSITION THEOREM

<div class="Reminder">
<p><strong>Reminder:</strong>
I will discuss only the Spectral Decomposition Theorem, but other methods could be used as Singular value decomposition.
</p>
</div>

An asymmetric matrix is a matrix that is equal to its transpose. Thus, 

<div class="equation">
$$ A \in \mathcal{M}_{p}(\mathbb{R}) $$
$$ \Rightarrow A^{\top} \in \mathcal{M}_{p}(\mathbb{R}) $$
$$\Rightarrow A = A^{\top}$$
</div>


<div class="Teorema">
<p><strong>Spectral Theorem (Symmetric matrix):</strong>
Let $A \in \mathbb{R}^{p \times p}$, then
</p>
</div>

<div class="lista">
  <ol type="I">
  <li> The eigenvalues $\lambda_{1}, \dots, \lambda_{p}$ are real;</li>
  <li> The eigenvectors of A associated with different eigenvalues are orthogonal to each other. **Then:**

<div class="equation">
$$\begin{aligned}
<\vec{v}_{i}, \vec{v}_{j}> &= \vec{v}_{i}^{\top} \vec{v}_{j} \\[10pt]
 &=  \sum_{t=1}^{p} \vec{v}_{ti}\vec{v}_{tj}  \\[10pt]
 &= \vec{v}_{1i}\vec{v}_{1j} + \dots+ \vec{v}_{pi}\vec{v}_{pj} \\[10pt]
 &= 0
\end{aligned}$$
</div>
with $\vec{v}_{j} = (v_{1j}, \dots, v_{pj})^{\top} \in \mathbb{R}^{p \times 1}$, $i \neq j$ and $i, j, k = 1, \dots, p$; 

<div class="Preposition">
<p><strong>Preposition 2</strong>.
An orthogonal set of non-zero vectors is linearly independent.
</p>
</div>
<div class="Reminder">
<p><strong>Reminder:</strong>

Linearly independent $\nrightarrow$ Orthogonal

Linearly independent $\leftarrow$ Orthogonal
</p>
</div>

</li>
<li> It is always possible to obtain a basis $\{e_{1}, \dots, e_{p}\}$ of orthonormal eigenvectors. **Then:**

<div class="equation">
$$ \begin{matrix}
 \cfrac{\vec{v}_{1}}{||\vec{v}_{1}||} = (e_{11}, e_{21}, \dots, e_{p1})^{\top} = e_{1} \\
  \cfrac{\vec{v}_{2}}{||\vec{v}_{2}||} = (e_{12}, e_{22}, \dots, e_{p2})^{\top} = e_{2} \\
  \vdots\\
 \cfrac{\vec{v}_{p}}{||\vec{v}_{p}||} = (e_{1p}, e_{2p}, \dots, e_{pp})^{\top} = e_{p}
\end{matrix}$$
</div>

<div class="equation">
$$ e_{i}^{\top} e_{j} = \delta_{ij} =   \begin{cases}
      0 & \text{if } i \neq j\\
      1 & \text{if } i = j
    \end{cases}$$
</div>

and $i, j = 1, \dots, p$;

</li>
<li> The relation is valid $P^{-1}AP = D$, so $P$ is defined as follows 

<div class="equation">
$$ P_{p \times p} =\begin{bmatrix}
 e_{.1} & \dots & e_{.p} \\
\end{bmatrix} = 
\begin{bmatrix}
 e_{11} &  e_{12} & \dots & e_{1p} \\
 e_{21} &  e_{22} & \dots & e_{2p} \\
 \vdots &  \vdots & \ddots & \vdots \\
 e_{p1} &  e_{p2} & \dots & e_{pp} \\
\end{bmatrix}$$ 
</div>

where $P$ is a orthonormal eigenvectors matrix.
<div class="Reminder">
<p><strong>Reminder:</strong>
Orthonormal eigenvectors matrix has the following properties
$P^{\top}P = I = PP^{\top}$ and $P^{-1} = P^{\top}$.
</p>
</div>

and $D$ matrix is defined by 

<div class="equation">
$$ D_{p \times p} =
\begin{bmatrix}
\lambda_{1} &         &       & \\
        &  \lambda_{2} &       & \\
        &         &\ddots & \\
        &         &       & \lambda_{p}\\
\end{bmatrix}$$ 
</div>

where $D$ is a diagonal matrix in which eigenvalues are diagonal elements;

</li>
</ol> 
</div>

<div class="Definition">
<p><strong>Definition 2</strong>. A quadratic form in $\mathbb{R}^{n}$ is a function $Q: \mathbb{R}^{p} \rightarrow \mathbb{R}$ of form $Q(y) = y^{\top}Ay$.
</p>
</div>

<div class="lista">
  <ol type="I">
    <li> $A$ is positive definite $\Leftrightarrow$ If $Q(y) > 0$, $\forall$ $y \in \mathbb{R}_{\small\setminus0}^{p}$.</li>
    <li> $A$ is positive semidefinite $\Leftrightarrow$ If $Q(y) \geq 0$, $\forall$ $y \in \mathbb{R}_{\small\setminus0}^{p}$.</li>
  </ol> 
</div>

Hence

<div class="lista">
  <ol type="I">
    <li> $A$ is positive definite, so $\lambda_{j} > 0$ for all $j = 1, \dots, p$.</li>
    <li> $A$ is positive semidefinite, so $\lambda_{j} \geq 0$ for all $j = 1, \dots, p$.</li>
  </ol> 
</div>

<div class="Important">
<p><strong>Important:</strong> If $A \in \mathbb{R}^{p \times p}$ and positive definite.
<div class="lista">
<ol type="I">
<li> A is non-singular $(\text{det}(A) > 0)$. **Exemplo:**
<div class="equation">
$$A = \begin{bmatrix}
            a & b \\
            b & d \\
          \end{bmatrix} \Rightarrow \text{det}(A) = ad - bb$$
</div>
<div class="equation">
$$\Rightarrow A^{-1} = \cfrac{1}{\text{det}(A)}\begin{bmatrix}
            d & - b \\
            - b & a \\
          \end{bmatrix}$$
</div>
</li>
<li> $A^{-1}$ is positive definite.</li>
</ol> 
</div>
</p>
</div>
<div class="Important">
<p>If $A \in \mathbb{R}^{p \times p}$ and positive semidefinite.
<div class="lista">
<ol type="I">
<li> All elements on the main diagonal are non-negative and $\mathrm{tr}(A) \geq 0$.</li>
<li> All principal minors $A$ are non-negative.</li>
</ol> 
</p>
</div>


# COVARIANCE MATRIX

The covariance matrix is

<div class="equation">
$$ \Sigma = \E((X - \E(X))(X - \E(X))^{\top})$$
</div>

where $X$ is a matrix of random variables from a sample; $x_{l} = (x_{l1}, \dots, x_{lp})^{\top}$ is a row vector of $X$ with $l = 1, \dots, n$; and sample mean vector 

<div class="equation">
$$ \overline{x} = \sum_{l = 1}^{n}\cfrac{x_{l}}{n} = (\overline{x}_{1}, \dots, \overline{x}_{p})^{\top}$$
</div>

Therefore, the sample covariance matrix is 

<div class="equation">
$$ S = \cfrac{1}{n - 1} \sum_{l = 1}^{n} (x_{l} - \overline{x})(x_{l} - \overline{x})^{\top}$$
</div>

Now let $\underset{ p \times 1}{e_{j}} = e_{.j} = (e_{1j}, \dots, e_{pj})^{\top} \in \mathbb{R}_{\small\setminus0}^{p}$ be any column vector. Thus

<div class="equation">
$$
e_{j}^{\top}Se_{j} = e_{j}^{\top}\bigg( \cfrac{1}{n - 1} \sum_{l = 1}^{n} (x_{l} - \overline{x})(x_{l} - \overline{x})^{\top} \bigg)e_{j}
$$
</div>

how $e_{j}$ does not dependent on $l$, we can move into the summation

<div class="equation">
$$
e_{j}^{\top}Se_{j} = \bigg( \cfrac{1}{n - 1} \sum_{l = 1}^{n} \underset{1 \times p}{e_{j}^{\top}}\underset{p \times 1}{(x_{l} - \overline{x})}\underset{1 \times p}{(x_{l} - \overline{x})^{\top}}\underset{p \times 1}{e_{j}} \bigg)
$$
</div>
<div class="equation">
$$
e_{j}^{\top}Se_{j} = \cfrac{1}{n - 1} \sum_{l = 1}^{n} \Big( (x_{l} - \overline{x})^{\top}e_{j} \Big)^{2} \geq 0 
$$
</div>

Thereby, the covariance matrix will always be a positive semidefinite matrix. Notice that $e_{j}^{\top}Se_{j}$ will only be zero whether $(x_{l} - \overline{x})^{\top}e_{j} = 0$ for all $l = 1, \dots, n$.

Another point that you need to take in mind is $\text{det}(S) = 0$, which means $e_{j}^{\top}Se_{j} = 0$; in this situation, $S$ is a singular matrix, i.e., there is no inverse. In other words, $S$ is ill-conditioned, and in my opinion, you need to begin trying if there is a collinearity problem. Or we can use [Moore-Penrose generalized inverse](https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse) to have a pseudoinverse matrix. 

# PRINCIPAL COMPONENT ANALYSIS

The PCA is a technique that will find directions in which the projection of data has the largest variance. The first direction is, by definition, called the first principal direction. To find the first one consider the following optimization problem $e_{1} \in \mathbb{R}^{p}$

<div class="equation">
$$
\begin{aligned}
& \underset{e_{1} \neq 0}{\text{max}}
& & \var(X e_{1}) = e_{1}^{\top}Se_{1} \\
& \text{subject to} & & ||e_{1}||^{2} = 1,  \\
\end{aligned}
$$
</div>

Using the formalization of the Lagrange multiplier method to $e_{1}$, we have

<div class="equation">
$$
L = e_{1}^{\top}Se_{1} - \lambda_{1}(e_{1}^{\top}e_{1} - 1) 
$$
</div>
<div class="equation">
$$
\begin{aligned}
\frac{\partial L}{\partial e_{1}} &= 2Se_{1} - 2\lambda_{1}e_{1} \Leftrightarrow 
e_{1}^{\top}Se_{1} = \lambda_{1} \\
 \\
\frac{\partial L}{\partial \lambda_{1}} &= e_{1}^{\top}e_{1} - 1 = 0  \Leftrightarrow e_{1}^{\top}e_{1} = 1\\
\end{aligned}
$$
</div>

It is not tough to see that $\lambda_{1}$ is the maximum variance for $\underset{ n \times 1}{Y_{1}} = \underset{ n \times p}{(X - \overline{x})} \underset{ p \times 1}{e_{1}}$.

Now, we need to find the second direction called the second principal direction. Consider the following optimization problem $e_{2} \in \mathbb{R}^{p}$

<div class="equation">
$$
\begin{aligned}
& \underset{e_{2} \neq 0}{\text{max}}
& & \var(X e_{2}) = e_{2}^{\top}Se_{2} \\
& \text{subject to} & & ||e_{2}||^{2} = 1,  \\
&                   & & e_{1}^{\top}e_{2} = 0, 
\end{aligned}
$$
</div>

Using the formalization of the Lagrange multiplier method, we have

<div class="equation">
$$
L = e_{2}^{\top}Se_{2} - \lambda_{2}(e_{2}^{\top}e_{2} - 1) - \phi e_{2}^{\top}e_{1}
$$
</div>
<div class="equation">
$$
\begin{aligned}
\frac{\partial L}{\partial e_{2}} &= 2Se_{2} - 2\lambda_{2}e_{2} - \phi e_{1} = 0 \\
 \\
\frac{\partial L}{\partial \lambda_{2}} &= e_{2}^{\top}e_{2} - 1 = 0 \Leftrightarrow  e_{2}^{\top}e_{2} = 1\\
\\
\frac{\partial L}{\partial \phi} &=  e_{2}^{\top}e_{1} = 0  \\
\end{aligned}
$$
</div>

So, we are going to find the value of $\lambda_{2}$ and $\phi$; then the first one will be $\phi$

<div class="equation">
$$
\begin{aligned}
2Se_{2} - 2\lambda_{2}e_{2} - \phi e_{1} &= 0 \\
2e_{1}^{\top}Se_{2} - 2\lambda_{2}e_{1}^{\top}e_{2} - \phi e_{1}^{\top}e_{1} &= 0 \\
2\lambda_{1}e_{1}^{\top}e_{2} - \phi &= 0\\
\phi &= 0
\end{aligned}
$$
</div>

where $e_{1}^{\top}e_{1} = 1, e_{1}^{\top}e_{2} = 0, \ \mbox{and} \ e_{1}^{\top}S = \lambda_{1}e_{1}^{\top}$; and to $\lambda_{2}$
<div class="equation">
$$
\begin{aligned}
\frac{\partial L}{\partial e_{2}} &= 2Se_{2} - 2\lambda_{2}e_{2} - \phi e_{1} = 0 \Leftrightarrow  e_{2}^{\top}Se_{2} = \lambda_{2}\\
 \\
\end{aligned}
$$
</div>

Now, $\lambda_{2}$ is the second maximum variance for $\underset{ n \times 1}{Y_{2}} = \underset{ n \times p}{(X - \overline{x})} \underset{ p \times 1}{e_{2}}$.

The results above can be extrapolated to $k$ principal components, where $k \leq p$. Then

<div class="equation">
$$
\begin{aligned}
\underset{ n \times 1}{Y_{1}} = \underset{ n \times p}{(X - \overline{x})} \underset{ p \times 1}{e_{1}} &\Leftrightarrow \var(Y_{1}) = \var(Xe_{1}) = e_{1}^{\top}Se_{1} = \lambda_{1}\\
\underset{ n \times 1}{Y_{2}} = \underset{ n \times p}{(X - \overline{x})} \underset{ p \times 1}{e_{2}} &\Leftrightarrow  \var(Y_{2}) = \var(Xe_{2}) = e_{2}^{\top}Se_{2} = \lambda_{2}\\
 \vdots \\
 \underset{ n \times 1}{Y_{k}} = \underset{ n \times p}{(X - \overline{x})} \underset{ p \times 1}{e_{k}} &\Leftrightarrow  \var(Y_{k}) = \var(Xe_{k}) = e_{k}^{\top}Se_{k} = \lambda_{k}\\
\end{aligned}
$$
</div>
and
<div class="equation">
$$
\begin{aligned}
\cov(Y_{i}, Y_{j}) = e_{i}^{\top}Se_{j} = \lambda_{i}e_{i}^{\top}e_{j} = 0, \ \forall \ i \neq j= 1, \dots, k \leq p \\
\end{aligned}
$$
</div>

where $\lambda_{1} \geq \lambda_{2} \geq \dots \geq \lambda_{k}$ and $Y_{i} \perp\!\!\!\!\perp Y_{j}$ are dependent on each other. Another point to note is

<div class="equation">
$$
\begin{aligned}
\sum_{t = 1}^{p} \var(\textbf{x}_{t}) = \sum_{t = 1}^{p} \var(Y_{t}) \Leftrightarrow  \sum_{t = 1}^{p} S_{tt} = \sum_{t = 1}^{p} \lambda_{t}\\
\end{aligned}
$$
</div>

in which $X = [\textbf{x}_{1} \ \dots \ \textbf{x}_{p}]$.

<div class="Important">
<p><strong>Important:</strong> If you want to work without centering the data $(\underset{ n \times p}{X})$ on the mean, the covariance matrix must be equal to $S = X^{\top}X / (n - 1)$.
</div>

<div class="equation">
$$
\begin{aligned}
\text{Information of}\ Y_{j} : \cfrac{\lambda_{j}}{\sum_{t = 1}^{p} \lambda_{t}}\times 100
\end{aligned}
$$
</div>

Of course, the first component will retain the most information on the variance of original data, and the second component will have the second most information on the variance of original data; in this way, it will follow until the $p$ component, which the last one will have the smallest information on variance of original data. 

Therefore, $k$ is the component number chosen by the researcher. It is common to have a situation where the total sample variance of original data can be explained from 80% to 90% by the first $k$ principal components. That is why we can replace the original p-variables with the $k$ component without much loss of information.

Now, I will introduce the correlation coefficient between $Y_{j}$ and $\textbf{x}_{i}$ for all $i, j = 1, \dots, p$. Then

<div class="equation">
$$
\begin{aligned}
{\Large r}_{\small Y_{j},\textbf{x}_{i}} = \cfrac{e_{ji}\sqrt{\lambda_{j}}}{\sqrt{S_{ii}}}
\end{aligned}
$$
</div>

With the measure of the correlation between $Y$ and $x$, you might decide which variable may stay in this specific component or remove. There is no magical formula that you might use to decide whether to leave or remove this specific variable. To me, if $|{\Large r}| < 0.25$, you perhaps need to think about how important this variable is in this component. And so, you decide. 

Generally, the principal component varies with the transformation on the scales. For example, imagine there is a variable $\textbf{x}_{1} \in (0, 1)$, but there is also a variable $\textbf{x}_{2} > 1000$ (minimum value). So, the principal component tends to give more importance to $\textbf{x}_{2}$ than $\textbf{x}_{1}$ just because of the difference in these scales. But it might not be the truth. In this case, you need to standardize your data.  

Assuming that,

<div class="equation">
$$U = \begin{bmatrix}
            S_{11} &        & &0\\
                   & S_{22} & &\\
                   &        & \ddots &\\
            0       &        &        &S_{pp} \\
          \end{bmatrix} \Rightarrow 
U^{-1/2} = \begin{bmatrix}
           \cfrac{1}{\sqrt{S_{11}}}  &        & &0\\
                   & \cfrac{1}{\sqrt{S_{22}}} & &\\
                   &        & \ddots &\\
            0       &        &        & \cfrac{1}{\sqrt{S_{pp}}} \\
          \end{bmatrix}$$ 
</div>

so 

<div class="equation">
$$
\begin{aligned}
Z &= \underset{ n \times p}{(X - \overline{x})} \underset{ p \times p}{U^{-1/2}}\\
\\
\cov(Z) &= U^{-1/2} \cov(X) U^{-1/2} = U^{-1/2} S U^{-1/2} = R\\
\\
S &= U^{-1/2} R U^{-1/2} \\
\end{aligned}
$$
</div>

where $R$ is correlation matrix of $X$. In this way, working with the standardized data is the same as with the correlation matrix. To standardized data, then

<div class="equation">
$$
\begin{aligned}
\text{Principal component:} &\ \ \ \underset{ n \times 1}{Y_{j}} = \underset{ n \times p}{Z} \underset{ p \times 1}{e_{j}} \Leftrightarrow \var(Y_{j}) = \lambda_{j}\\
\\
\text{Total variance:} &\ \ \ \sum_{t = 1}^{p} \var(\textbf{z}_{t}) = \sum_{t = 1}^{p} \var(Y_{t}) \Leftrightarrow  \sum_{t = 1}^{p} R_{tt} = \sum_{t = 1}^{p} \lambda_{t} = p\\
\\
\text{Information of}\ Y_{j} :& \ \ \ \cfrac{\lambda_{j}}{p}\times 100 \\
\\
\text{Correlation coefficient:}&\ \ \ {\Large r}_{\small Y_{j},\textbf{x}_{i}} = e_{ji}\sqrt{\lambda_{j}}
\end{aligned}
$$
</div>

Everything that has already been said for the covariance matrix also applies to the correlation matrix.

Data application will come soon!


 
# REFERENCES

<div id="refs">
---
nocite: '@*'
---
</div>

</div>
</section>